# "FASTAI Generate Wiki-IMDB Language model"
> "Fatpages blog"

- toc:true- branch: master
- badges: true
- comments: true
- author: Conwyn
- categories: [fastai,language,encoder]

%matplotlib inline
%reload_ext autoreload

!pip install -Uqq fastbook
import fastbook
fastbook.setup_book()
from fastai import *
from fastai.text.all import *

Download the FastAI IMDB entries

path=untar_data(URLs.IMDB) #/root/.fastai/data/imdb
get_imdb = partial(get_text_files,folders=['train','test','unsup'])

Build a datablock

dls_lm = DataBlock(
   blocks = TextBlock.from_folder(path,is_lm=True),
    get_items=get_imdb,splitter=RandomSplitter(0.1)
).dataloaders(path,path=path,bs=128,seq_len=80)

Save the Datablock for later when we require the word-token mapping when we use the encoder


import pickle
pickle.dump( dls_lm , open( "savelm3.p", "wb" ) )
!cp /content/savelm3.p /content/gdrive/MyDrive



#hide
import pickle
!cp  /content/gdrive/MyDrive/savelm3.p /content
dls_lm = pickle.load( open( "/content/savelm3.p", "rb" ) )
#learnlm = language_model_learner(dls_lm,AWD_LSTM,drop_mult=0.3,metrics=[accuracy,Perplexity()]).to_fp16()

Create the learner

learnlm = language_model_learner(dls_lm,AWD_LSTM,drop_mult=0.3,metrics=[accuracy, Perplexity()]).to_fp16()

Fit the learner and optionally save the model

learnlm.fit_one_cycle(1,2e-2)

learnlm.save('1epochF')

!cp /root/.fastai/data/imdb/models/1epochE.pth /content/gdrive/MyDrive

Note it uses a model subfolder within the path

print(path)



Unfreeze the model and fit. Note this took over five hours. Save the encoder for later

learnlm.unfreeze()
learnlm.fit_one_cycle(10,2e-3) # 5 hours

learnlm.save_encoder('finetunedF')
!cp /root/.fastai/data/imdb/models/finetunedF.pth /content/gdrive/MyDrive
#learnlm.load_encoder('finetunedF')
